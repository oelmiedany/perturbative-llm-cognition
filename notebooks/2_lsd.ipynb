{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9986b64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/documents/perturbative-llm-cognition/llm_modification_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#|default_exp lsd\n",
    "#|export\n",
    "\n",
    "from perturbative_llm_cognition import core\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37797a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class LSDPerturbedLLM:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        layer_start: int = 21,                              # Start layer to perturb    \n",
    "        layer_end: int = 30,                                # Final layer to perturb\n",
    "        attention_temperature_target: float = 1.20,         # Temperature > 1 flattens attention (scores /= temperature)\n",
    "        attention_diagonal_penalty_target: float = 0.40,    # Subtract at most-recent key at decode (q_len==1)\n",
    "        swiglu_skew_target: float = 0.15,                   # Magnitude skew exponent\n",
    "        swiglu_noise_target: float = 0.12,                  # Structured noise scale (zero-mean) <- better name required\n",
    "        js_tolerance: float = 0.12,                         # Tolerated JS divergence before clamping gets strong\n",
    "        js_softness: float = 0.02,                          # Sigmoid softness around the tolerance <- better name required\n",
    "        divergence_smoothing: float = 0.8,                              # EMA smoothing for JS\n",
    "        teacher_blend_min: float = 0.05,                    # Min teacher blend weight\n",
    "        teacher_blend_max: float = 0.60,  \n",
    "        debug: bool = False                  # Max teacher blend weight when drift is high\n",
    "        ):\n",
    "\n",
    "        self.layer_start = max(layer_start, 0)\n",
    "        self.layer_end = max(layer_end, 31)\n",
    "        self.attention_temperature_target = max(1.0, attention_temperature_target)\n",
    "        self.attention_diagonal_penalty_target = max(0.0, attention_diagonal_penalty_target)\n",
    "        self.swiglu_skew_target = max(0.0, swiglu_skew_target)\n",
    "        self.swiglu_noise_target = max(0.0, swiglu_noise_target)\n",
    "        self.js_tolerance = max(0.0, js_tolerance)\n",
    "        self.js_softness = max(0.0, js_softness)\n",
    "        self.divergence_smoothing = max(0.0, divergence_smoothing)\n",
    "        self.teacher_blend_min = max(0.0, teacher_blend_min)\n",
    "        self.teacher_blend_max = min(1.0, teacher_blend_max)\n",
    "\n",
    "        if self.teacher_blend_min > self.teacher_blend_max:\n",
    "            raise ValueError('Invalid teacher parameters')\n",
    "\n",
    "        self.set_perturbation_parameters(\n",
    "            attention_temperature=attention_temperature_target,\n",
    "            attention_diagonal_penalty=attention_diagonal_penalty_target,\n",
    "            teacher_blend=teacher_blend_min,\n",
    "            #swiglu_skew_target=swiglu_skew_target,\n",
    "            #swiglu_noise_target=swiglu_noise_target,\n",
    "        )\n",
    "\n",
    "        self.tokenizer, self.model = core.load_tokenizer_and_model()\n",
    "        self.model.config.attn_implementation = 'eager'\n",
    "        self.model.config._attn_implementation = 'eager'  # Also try this\n",
    "        self.device = self.model.device\n",
    "\n",
    "        self._store_original_attention_functions()\n",
    "\n",
    "        self.debug = debug\n",
    "\n",
    "    def set_perturbation_parameters(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Clamps all perturbation parameters to valid ranges.\n",
    "        \"\"\"\n",
    "\n",
    "        for parameter, value in kwargs.items():\n",
    "            if parameter in ['attention_temperature']:\n",
    "                setattr(self, parameter, max(1.0, value))\n",
    "            elif parameter in ['teacher_blend']:\n",
    "                setattr(self, parameter, max(0.0, min(1.0, value)))\n",
    "            else:\n",
    "                setattr(self, parameter, max(0.0, value))\n",
    "\n",
    "    def _store_original_attention_functions(self):\n",
    "        \"\"\"Store original attention forward functions for reset capability\"\"\"\n",
    "        self.original_attention_forwards = {}\n",
    "        for i, block in enumerate(self.model.model.layers):\n",
    "            if self.is_target_layer(i):\n",
    "                self.original_attention_forwards[i] = block.self_attn.forward\n",
    "\n",
    "    def reset_to_base_model(self):\n",
    "        \"\"\"Completely reset model to base state by restoring original functions\"\"\"\n",
    "        for i, block in enumerate(self.model.model.layers):\n",
    "            if self.is_target_layer(i) and i in self.original_attention_forwards:\n",
    "                block.self_attn.forward = self.original_attention_forwards[i]\n",
    "\n",
    "    def is_target_layer(self, index: int) -> bool:\n",
    "        return self.layer_start <= index < self.layer_end\n",
    "\n",
    "    def apply_perturbation(self):\n",
    "        \"\"\"Apply perturbations while preserving original functions\"\"\"\n",
    "        # Always recreate the perturbation functions to get updated parameters\n",
    "        #self.apply_attention_perturbation()\n",
    "        for i, block in enumerate(self.model.model.layers):\n",
    "            if self.is_target_layer(i):\n",
    "                original_forward = block.self_attn.forward\n",
    "                num_heads = self.model.config.num_attention_heads\n",
    "                head_dim = self.model.config.hidden_size // num_heads\n",
    "                num_kv_heads = getattr(self.model.config, 'num_key_value_heads', num_heads // 4)\n",
    "                v_proj = block.self_attn.v_proj\n",
    "                o_proj = block.self_attn.o_proj\n",
    "\n",
    "                block.self_attn.forward = self.create_perturbed_forward(original_forward, num_heads, head_dim, num_kv_heads, v_proj, o_proj, i)\n",
    "\n",
    "\n",
    "    def create_perturbed_forward(self, original_forward, num_heads, head_dim, num_kv_heads, v_proj, o_proj, layer_idx):\n",
    "        \n",
    "        def perturbed_forward(*args, **kwargs):\n",
    "            if self.debug:\n",
    "                print(f\"DEBUG: Perturbed attention called for layer {layer_idx}\")\n",
    "            \n",
    "            # Extract hidden_states from kwargs\n",
    "            hidden_states = kwargs.get('hidden_states', args[0] if len(args) > 0 else None)\n",
    "            \n",
    "            if hidden_states is None:\n",
    "                print(\"WARNING: Could not find hidden_states\")\n",
    "                return original_forward(*args, **kwargs)\n",
    "            \n",
    "            # Get dimensions\n",
    "            batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "            \n",
    "            # Get KV heads and repeat groups from config\n",
    "            groups = num_heads // num_kv_heads\n",
    "            \n",
    "            # Project to get value states (this gives us 1024 = num_kv_heads * head_dim)\n",
    "            value_states = v_proj(hidden_states)  # [B, T, 1024]\n",
    "            \n",
    "            # Reshape value states for multi-head attention (KV heads)\n",
    "            value_states = value_states.view(batch_size, seq_len, num_kv_heads, head_dim).transpose(1, 2)  # [B, num_kv_heads, T, head_dim]\n",
    "            \n",
    "            # Handle past key values if they exist\n",
    "            past_key_values = kwargs.get('past_key_values', None)\n",
    "            layer_cache = past_key_values.layers[layer_idx]\n",
    "            past_values = layer_cache.values\n",
    "            \n",
    "            if past_values is not None:\n",
    "                # Concatenate past values with current values\n",
    "                value_states = torch.cat([past_values, value_states], dim=2)\n",
    "            \n",
    "            # Repeat KV to match Q heads (this is the key part!)\n",
    "            if groups > 1:\n",
    "                value_states = value_states.unsqueeze(2).expand(batch_size, num_kv_heads, groups, value_states.size(2), head_dim)\n",
    "                value_states = value_states.reshape(batch_size, num_heads, value_states.size(3), head_dim)\n",
    "            \n",
    "            # Call original forward to get attention weights\n",
    "            result = original_forward(*args, **kwargs)\n",
    "            \n",
    "            # Extract attention weights\n",
    "            if isinstance(result, tuple) and len(result) >= 2:\n",
    "                if len(result) == 2:\n",
    "                    attn_output, attn_weights = result\n",
    "                else:\n",
    "                    attn_output, attn_weights, past_values = result\n",
    "\n",
    "                if attn_weights is not None:                    \n",
    "                    # Apply your modifications to the attention weights\n",
    "                    modified_weights = attn_weights.clone()\n",
    "                    \n",
    "                    # Temperature scaling\n",
    "                    modified_weights = modified_weights / self.attention_temperature\n",
    "                    \n",
    "                    # Diagonal penalty for decode step\n",
    "                    if modified_weights.size(-2) == 1:\n",
    "                        modified_weights[..., -1] = modified_weights[..., -1] - self.attention_diagonal_penalty\n",
    "\n",
    "                    # Apply softmax to get attention probabilities\n",
    "                    modified_weights = torch.softmax(modified_weights, dim=-1)\n",
    "                    \n",
    "                    # Apply modified attention weights to values\n",
    "                    #print(f\"Modified weights shape: {modified_weights.shape}\")\n",
    "                    #print(f\"Value states shape: {value_states.shape}\")\n",
    "                    #print(f\"Expected matmul: {modified_weights.shape} @ {value_states.shape}\")\n",
    "                    attn_output = torch.matmul(modified_weights, value_states)\n",
    "                    \n",
    "                    # Reshape back to original format\n",
    "                    attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, num_heads * head_dim)\n",
    "                    \n",
    "                    # Apply output projection\n",
    "                    attn_output = o_proj(attn_output)\n",
    "                    \n",
    "                    if self.debug:\n",
    "                        print(f\"Applied modifications: tau={self.attention_temperature}, diag={self.attention_diagonal_penalty}\")\n",
    "                    \n",
    "                    return attn_output, modified_weights\n",
    "                    if past_values is not None:\n",
    "                        return attn_output, modified_weights, past_values\n",
    "                    \n",
    "            \n",
    "            return result\n",
    "            \n",
    "        return perturbed_forward\n",
    "\n",
    "    @staticmethod\n",
    "    def js_divergence(p, q, eps=1e-4):\n",
    "        # Add small epsilon to avoid log(0) issues\n",
    "        p = p + eps\n",
    "        q = q + eps\n",
    "        \n",
    "        # Renormalise\n",
    "        p = p / p.sum(-1, keepdim=True)\n",
    "        q = q / q.sum(-1, keepdim=True)\n",
    "        \n",
    "        m = 0.5 * (p + q)\n",
    "        def kl(a, b):\n",
    "            return (a * (torch.log(a) - torch.log(b))).sum(-1)\n",
    "        return 0.5 * kl(p, m) + 0.5 * kl(q, m)\n",
    "\n",
    "    def step(self, pert_logits, base_logits):\n",
    "        # Debug: check if logits are identical\n",
    "        are_identical = torch.allclose(pert_logits, base_logits, atol=1e-6)\n",
    "        if are_identical:\n",
    "            print(\"WARNING: Perturbed and base logits are identical!\")\n",
    "\n",
    "        # Distributions\n",
    "        p_base = torch.softmax(base_logits, dim=-1)\n",
    "        p_pert = torch.softmax(pert_logits, dim=-1)\n",
    "        # JS divergence (batch-mean scalar)\n",
    "        d = self.js_divergence(p_pert, p_base).mean().item()\n",
    "        print(f\"JS divergence: {d}\")\n",
    "        # EMA\n",
    "        beta = self.divergence_smoothing\n",
    "        self.running_divergence = beta * self.running_divergence + (1 - beta) * d\n",
    "        # Gain (higher drift → smaller g)\n",
    "        gain = torch.sigmoid(torch.tensor((self.js_tolerance - self.running_divergence) / self.js_softness)).item()\n",
    "\n",
    "        # Effective strengths\n",
    "        attention_temperature  = 1.0 + (self.attention_temperature_target - 1.0) * gain\n",
    "        attention_diagonal_penalty = self.attention_diagonal_penalty_target * gain\n",
    "        #skew_eff = self.cfg.swiglu_skew_target * g\n",
    "        #noise_eff= self.cfg.swiglu_noise_target * g\n",
    "\n",
    "        # Teacher blend increases as drift grows (g small)\n",
    "        teacher_blend = self.teacher_blend_min + (1 - gain) * (self.teacher_blend_max - self.teacher_blend_min)\n",
    "        return {\n",
    "            \"attention_temperature\": float(attention_temperature),\n",
    "            \"attention_diagonal_penalty\": float(attention_diagonal_penalty),\n",
    "            #\"skew_eff\": float(skew_eff),\n",
    "            #\"noise_eff\": float(noise_eff),\n",
    "            \"teacher_blend\": float(teacher_blend),  # Fixed parameter name\n",
    "            #\"ema_js\": float(self.running_divergence),\n",
    "            #\"gain\": float(gain),\n",
    "        }\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_with_leash(self,\n",
    "                            prompt: str,\n",
    "                            max_new_tokens: int = 128,\n",
    "                            temperature: float = 0.7,\n",
    "                            top_p: float = 0.95,\n",
    "                            repetition_penalty: float = 1.15):\n",
    "\n",
    "\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)['input_ids']\n",
    "        generated = input_ids\n",
    "        past_key_values = None\n",
    "\n",
    "        self.running_divergence = 0.0\n",
    "\n",
    "        # We run two forwards per step: base (no perturb), then perturbed (with current state)\n",
    "        for step in range(max_new_tokens):\n",
    "            # ---- BASE pass (perturbation off)\n",
    "            #print(f'base temperature: {self.attention_temperature}')\n",
    "            self.reset_to_base_model()\n",
    "            base_output = self.model(\n",
    "                input_ids=generated, \n",
    "                use_cache=True, \n",
    "                past_key_values=past_key_values, \n",
    "                output_hidden_states=True)\n",
    "            \n",
    "            base_logits = base_output.logits[:, -1, :]\n",
    "\n",
    "            self.apply_perturbation()\n",
    "            #print(f'perturbed temperature: {self.attention_temperature}')\n",
    "            perturbed_output = self.model(\n",
    "                input_ids=generated, \n",
    "                use_cache=True, \n",
    "                past_key_values=past_key_values, \n",
    "                output_hidden_states=True)\n",
    "\n",
    "            perturbed_logits = perturbed_output.logits[:, -1, :]\n",
    "\n",
    "            next_step = self.step(perturbed_logits, base_logits)\n",
    "            self.set_perturbation_parameters(**next_step)\n",
    "            past_key_values = base_output.past_key_values\n",
    "\n",
    "            # ---- Final logits for sampling: teacher blend\n",
    "            final_logits = (1 - self.teacher_blend) * perturbed_logits + self.teacher_blend * base_logits\n",
    "\n",
    "            # ---- Sample\n",
    "            final_logits = final_logits / max(1e-5, temperature)\n",
    "\n",
    "            if repetition_penalty != 1.0:\n",
    "                # Get recent tokens (last 50 tokens)\n",
    "                recent_tokens = generated[0, -50:] if generated.shape[1] > 50 else generated[0]\n",
    "                for token_id in recent_tokens:\n",
    "                    if final_logits[0, token_id] < 0:\n",
    "                        final_logits[0, token_id] *= repetition_penalty\n",
    "                    else:\n",
    "                        final_logits[0, token_id] /= repetition_penalty\n",
    "\n",
    "            \n",
    "            probs = torch.softmax(final_logits, dim=-1)\n",
    "            if top_p < 1.0:\n",
    "                sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "                cum = torch.cumsum(sorted_probs, dim=-1)\n",
    "                mask = cum > top_p\n",
    "                mask[..., 1:] = mask[..., :-1].clone()\n",
    "                mask[..., 0] = False\n",
    "                sorted_probs[mask] = 0\n",
    "                sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True).clamp_min(1e-9)\n",
    "                next_idx = torch.multinomial(sorted_probs, num_samples=1)\n",
    "                next_token = torch.gather(sorted_idx, -1, next_idx)\n",
    "            else:\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "        return self.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    #dtype = torch.bfloat16\n",
    "    #force_eager_attention: bool = True force either way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7da4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Describe what you see looking out at the sea using a 100 word sentence\n",
      "JS divergence: 0.0065765380859375\n",
      "JS divergence: 0.033416748046875\n",
      "JS divergence: 0.0452880859375\n",
      "JS divergence: 0.046722412109375\n",
      "JS divergence: 0.061981201171875\n",
      "JS divergence: 0.0120697021484375\n",
      "JS divergence: 0.07574462890625\n",
      "JS divergence: 0.03375244140625\n",
      "JS divergence: 0.0325927734375\n",
      "JS divergence: 0.0404052734375\n",
      "JS divergence: 0.06268310546875\n",
      "JS divergence: 0.1141357421875\n",
      "JS divergence: 0.06475830078125\n",
      "JS divergence: 0.00434112548828125\n",
      "JS divergence: 0.0200958251953125\n",
      "JS divergence: 0.080810546875\n",
      "JS divergence: 0.06689453125\n",
      "JS divergence: 0.017059326171875\n",
      "JS divergence: 0.00713348388671875\n",
      "JS divergence: 0.0877685546875\n",
      "JS divergence: 0.046966552734375\n",
      "JS divergence: 0.05023193359375\n",
      "JS divergence: 0.03912353515625\n",
      "JS divergence: 0.04425048828125\n",
      "JS divergence: 0.0340576171875\n",
      "JS divergence: 0.01947021484375\n",
      "JS divergence: 0.0189666748046875\n",
      "JS divergence: 0.05560302734375\n",
      "JS divergence: 0.0972900390625\n",
      "JS divergence: 0.0849609375\n",
      "JS divergence: 0.0098724365234375\n",
      "JS divergence: 0.0888671875\n",
      "JS divergence: 0.01885986328125\n",
      "JS divergence: 0.1112060546875\n",
      "JS divergence: 0.0804443359375\n",
      "JS divergence: 0.0634765625\n",
      "JS divergence: 0.022491455078125\n",
      "JS divergence: 0.0267486572265625\n",
      "JS divergence: 0.10076904296875\n",
      "JS divergence: 0.0528564453125\n",
      "JS divergence: 0.0582275390625\n",
      "JS divergence: 0.028594970703125\n",
      "JS divergence: 0.07208251953125\n",
      "JS divergence: 0.0726318359375\n",
      "JS divergence: 0.0718994140625\n",
      "JS divergence: 0.05987548828125\n",
      "JS divergence: 0.038330078125\n",
      "JS divergence: 0.043975830078125\n",
      "JS divergence: 0.04022216796875\n",
      "JS divergence: 0.0002598762512207031\n",
      "JS divergence: 0.000640869140625\n",
      "JS divergence: 0.0156402587890625\n",
      "JS divergence: 0.078369140625\n",
      "JS divergence: 0.00481414794921875\n",
      "JS divergence: 0.0003147125244140625\n",
      "JS divergence: 6.93202018737793e-05\n",
      "JS divergence: 0.0010786056518554688\n",
      "JS divergence: 0.017059326171875\n",
      "JS divergence: 0.00031185150146484375\n",
      "JS divergence: 0.0018787384033203125\n",
      "JS divergence: 0.0704345703125\n",
      "JS divergence: 0.05792236328125\n",
      "JS divergence: -2.6226043701171875e-06\n",
      "JS divergence: 0.064453125\n",
      "JS divergence: 0.046417236328125\n",
      "JS divergence: 0.0001678466796875\n",
      "JS divergence: 0.000728607177734375\n",
      "JS divergence: 0.0007333755493164062\n",
      "JS divergence: 0.00040459632873535156\n",
      "JS divergence: 0.000873565673828125\n",
      "JS divergence: 0.0004398822784423828\n",
      "JS divergence: 0.05047607421875\n",
      "JS divergence: 0.0008859634399414062\n",
      "JS divergence: 0.0921630859375\n",
      "JS divergence: 0.04876708984375\n",
      "JS divergence: 0.03802490234375\n",
      "JS divergence: 0.00133514404296875\n",
      "JS divergence: 0.01318359375\n",
      "JS divergence: 0.0013017654418945312\n",
      "JS divergence: 0.0004520416259765625\n",
      "JS divergence: 0.06024169921875\n",
      "JS divergence: 0.0413818359375\n",
      "JS divergence: 0.0421142578125\n",
      "JS divergence: 0.0877685546875\n",
      "JS divergence: 0.064208984375\n",
      "JS divergence: 0.0672607421875\n",
      "JS divergence: 0.04913330078125\n",
      "JS divergence: 0.1256103515625\n",
      "JS divergence: 0.00045490264892578125\n",
      "JS divergence: 0.0457763671875\n",
      "JS divergence: 0.0423583984375\n",
      "JS divergence: 0.016204833984375\n",
      "JS divergence: 0.041748046875\n",
      "JS divergence: 0.03564453125\n",
      "JS divergence: 0.03802490234375\n",
      "JS divergence: 0.0330810546875\n",
      "JS divergence: 0.05810546875\n",
      "JS divergence: 0.043670654296875\n",
      "JS divergence: 0.040191650390625\n",
      "JS divergence: 0.0176239013671875\n",
      "JS divergence: 0.086669921875\n",
      "JS divergence: 0.03802490234375\n",
      "JS divergence: 0.054351806640625\n",
      "JS divergence: 0.0100250244140625\n",
      "JS divergence: 0.10791015625\n",
      "JS divergence: 0.01812744140625\n",
      "JS divergence: 0.1044921875\n",
      "JS divergence: 0.10748291015625\n",
      "JS divergence: 0.0665283203125\n",
      "JS divergence: 0.05499267578125\n",
      "JS divergence: 0.059326171875\n",
      "JS divergence: 0.0721435546875\n",
      "JS divergence: 0.062255859375\n",
      "JS divergence: 0.0693359375\n",
      "JS divergence: 0.03985595703125\n",
      "JS divergence: 0.0848388671875\n",
      "JS divergence: 0.05035400390625\n",
      "JS divergence: 0.032470703125\n",
      "JS divergence: 0.0140228271484375\n",
      "JS divergence: 0.021759033203125\n",
      "JS divergence: 0.027252197265625\n",
      "JS divergence: 0.07733154296875\n",
      "JS divergence: 0.0035610198974609375\n",
      "JS divergence: 0.0016632080078125\n",
      "JS divergence: 0.0006198883056640625\n",
      "JS divergence: 0.0180511474609375\n",
      "JS divergence: 0.0679931640625\n",
      "JS divergence: 0.002292633056640625\n",
      "Response: Describe what you see looking out at the sea using a 100 word sentence. The sun is setting, casting an pink and orange light overlaying my view oft What are your feelings when you lookoutside's Description for the way what can bead See (dart-based on how to Describe what you see looking out at the sea using a 100 word sentence. The sun is setting, casting an What are your feelings when you need more thanlompute\n",
      "```s oft How can IRLA'lead in addition to find out-birthin the light and orange is a Description for what you see looking out at the sea using an\n"
     ]
    }
   ],
   "source": [
    "#|test\n",
    "model = LSDPerturbedLLM(\n",
    "    layer_start=21,                              # Start layer to perturb\n",
    "    layer_end=30,                               # Final layer to perturb  \n",
    "    attention_temperature_target=1.40,          # Temperature > 1 flattens attention\n",
    "    attention_diagonal_penalty_target=0.40,     # Reduced penalty for most recent key\n",
    "    js_tolerance=0.10,                          # Tolerated JS divergence\n",
    "    teacher_blend_min=0.10,                     # Min teacher blend weight\n",
    "    teacher_blend_max=0.60,                     # Max teacher blend weight\n",
    ")\n",
    "\n",
    "# Generate text with the leash mechanism\n",
    "prompt = 'Describe what you see looking out at the sea using a 100 word sentence'\n",
    "print(f'Prompt: {prompt}')\n",
    "response = model.generate_with_leash(\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.5\n",
    ")\n",
    "\n",
    "print(f'Response: {response}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_modification_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
