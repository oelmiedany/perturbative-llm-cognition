{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp lsd\n",
    "#|export\n",
    "\n",
    "from perturbative_llm_cognition import core\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import types\n",
    "\n",
    "from torch import nn\n",
    "from transformers.models.mistral.modeling_mistral import (\n",
    "    apply_rotary_pos_emb,\n",
    "    repeat_kv,\n",
    ")\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a37797a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class LSDPerturbedLLM:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        layer_start: int = 21,                              # Start layer to perturb    \n",
    "        layer_end: int = 27,      \n",
    "        attention_scaling_factor: float = 1.4,\n",
    "        attention_noise: float = 0.3,\n",
    "        attention_diagonal_penalty:float = 0.2, \n",
    "        attention_probability_smoothing_factor: float = 0.5,\n",
    "        js_tolerance: float = 0.2,                         # Tolerated JS divergence before clamping gets strong\n",
    "        teacher_blend_min: float = 0.10,                    # Min teacher blend weight\n",
    "        teacher_blend_max: float = 0.50,  \n",
    "        debug: bool = False                  # Max teacher blend weight when drift is high\n",
    "        ):\n",
    "\n",
    "        self.layer_start = max(layer_start, 0)\n",
    "        self.layer_end = max(layer_end, 31)\n",
    "        self.js_tolerance = max(0.0, js_tolerance)\n",
    "        #self.js_softness = max(0.0, js_softness)\n",
    "        self.teacher_blend_min = max(0.0, teacher_blend_min)\n",
    "        self.teacher_blend_max = min(1.0, teacher_blend_max)\n",
    "\n",
    "        if self.teacher_blend_min > self.teacher_blend_max:\n",
    "            raise ValueError('Invalid teacher parameters')\n",
    "\n",
    "\n",
    "        self.attention_scaling_factor = attention_scaling_factor\n",
    "        self.attention_noise = attention_noise\n",
    "        self.attention_diagonal_penalty = attention_diagonal_penalty\n",
    "        self.attention_probability_smoothing_factor = attention_probability_smoothing_factor\n",
    "\n",
    "\n",
    "        self.tokenizer, self.model = core.load_tokenizer_and_model()\n",
    "        self.model.config.attn_implementation = 'eager'\n",
    "        self.model.config._attn_implementation = 'eager'  # Also try this\n",
    "        self.device = self.model.device\n",
    "\n",
    "        self._store_original_attention_functions()\n",
    "\n",
    "        try:\n",
    "            self.stop_words = self.tokenizer(' '.join(stopwords.words('english')), return_tensors='pt'\n",
    "            ).to(self.device)['input_ids']\n",
    "        except LookupError:\n",
    "            nltk.download('stopwords')\n",
    "            self.stop_words = self.tokenizer(' '.join(stopwords.words('english')), return_tensors='pt'\n",
    "            ).to(self.device)['input_ids']\n",
    "\n",
    "        self.debug = debug\n",
    "\n",
    "    def _store_original_attention_functions(self):\n",
    "        \"\"\"Store original attention forward functions for reset capability\"\"\"\n",
    "        self.original_attention_forwards = {}\n",
    "        for i, block in enumerate(self.model.model.layers):\n",
    "            if self.is_target_layer(i):\n",
    "                self.original_attention_forwards[i] = block.self_attn.forward\n",
    "\n",
    "    def reset_to_base_model(self):\n",
    "        \"\"\"Completely reset model to base state by restoring original functions\"\"\"\n",
    "        for i, block in enumerate(self.model.model.layers):\n",
    "            if self.is_target_layer(i) and i in self.original_attention_forwards:\n",
    "                block.self_attn.forward = self.original_attention_forwards[i]\n",
    "\n",
    "    def is_target_layer(self, index: int) -> bool:\n",
    "        return self.layer_start <= index < self.layer_end\n",
    "\n",
    "    def apply_perturbation(self):\n",
    "        \"\"\"Apply perturbations while preserving original functions\"\"\"\n",
    "        # Always recreate the perturbation functions to get updated parameters\n",
    "        for i, block in enumerate(self.model.model.layers):\n",
    "            if self.is_target_layer(i):\n",
    "                layer = block.self_attn\n",
    "                self.create_perturbed_forward(layer, i)\n",
    "\n",
    "    def create_perturbed_forward(self, layer, layer_idx):\n",
    "        \"\"\"Create a perturbed forward function following the same logic as the provided code\"\"\"\n",
    "        attention_scaling_factor = self.attention_scaling_factor\n",
    "        attention_noise = self.attention_noise\n",
    "        diag_penalty = self.attention_diagonal_penalty\n",
    "        attention_probability_smoothing_factor = self.attention_probability_smoothing_factor\n",
    "        debug = self.debug\n",
    "        \n",
    "        def forward(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            attention_mask=None,\n",
    "            position_ids=None,\n",
    "            past_key_value=None,\n",
    "            output_attentions=False,\n",
    "            use_cache=False,\n",
    "            cache_position=None,\n",
    "        ):\n",
    "            b, q_len, _ = hidden_states.size()\n",
    "            \n",
    "            # Q K V projections\n",
    "            q = self.q_proj(hidden_states).view(b, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            k = self.k_proj(hidden_states).view(b, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "            v = self.v_proj(hidden_states).view(b, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "            \n",
    "            # RoPE (rotary positional embeddings)\n",
    "            seq_len = v.shape[-2]  # or q_len, whichever matches your attention shape\n",
    "            cos, sin = self.rotary_emb(\n",
    "                v,\n",
    "                position_ids if position_ids is not None else torch.arange(seq_len, device=v.device).unsqueeze(0))\n",
    "            \n",
    "            q, k = apply_rotary_pos_emb(q, k, cos, sin, position_ids=None)\n",
    "            \n",
    "            # Cache update\n",
    "            if past_key_value is not None:\n",
    "                k, v = past_key_value.update(k, v, layer_idx, {'sin': sin, 'cos': cos, 'cache_position': cache_position})\n",
    "            \n",
    "            # Expand KV\n",
    "            k = repeat_kv(k, self.num_key_value_groups)\n",
    "            v = repeat_kv(v, self.num_key_value_groups)\n",
    "            \n",
    "            # Attention logits\n",
    "            attention_scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "            if attention_mask is not None:\n",
    "                attention_scores = attention_scores + attention_mask[:, :, :, : k.size(-2)]\n",
    "            \n",
    "            # === Perturbation modifications ===\n",
    "            attention_scores = attention_scores / attention_scaling_factor\n",
    "\n",
    "            # Add noise for exploration\n",
    "            noise = torch.randn_like(attention_scores) * attention_noise\n",
    "            attention_scores = attention_scores + noise\n",
    "\n",
    "            # Penalise diagonal/self-attention to look elsewhere\n",
    "            if attention_scores.shape[-2] == 1:\n",
    "                attention_scores[..., -1] = attention_scores[..., -1] - diag_penalty\n",
    "\n",
    "            # Softmax\n",
    "            attention_probabilities = nn.functional.softmax(attention_scores, dim=-1).to(q.dtype)\n",
    "\n",
    "            # Flatten distribution for more uniform attention\n",
    "            attention_probabilities = attention_probabilities ** attention_probability_smoothing_factor\n",
    "            attention_probabilities = attention_probabilities / attention_probabilities.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            attention_probabilities = nn.functional.dropout(attention_probabilities, p=self.attention_dropout, training=self.training)\n",
    "            \n",
    "            # Weighted sum\n",
    "            attention_output = torch.matmul(attention_probabilities, v).transpose(1, 2).contiguous().view(b, q_len, -1)\n",
    "            attention_output = self.o_proj(attention_output)\n",
    "            \n",
    "            if debug:\n",
    "                print(f'[L{layer_idx}] attention_scaling_factor={attention_scaling_factor} attention_noise={attention_noise} diag={diag_penalty}')\n",
    "            \n",
    "            return (\n",
    "                attention_output,\n",
    "                (attention_probabilities if output_attentions else None),\n",
    "                (past_key_value if use_cache else None),\n",
    "            )\n",
    "        \n",
    "        # Bind the forward function to the layer\n",
    "        layer.forward = types.MethodType(forward, layer)\n",
    "        return layer\n",
    "\n",
    "    @staticmethod\n",
    "    def js_divergence(p, q, epsilon=1e-6, threshold=1e-6):\n",
    "        mask = (p > threshold) | (q > threshold)\n",
    "    \n",
    "        if mask.sum() == 0:\n",
    "            raise ValueError('No valid probabilities found in input distributions')\n",
    "        \n",
    "        p = p[mask]\n",
    "        q = q[mask]\n",
    "\n",
    "        # Add small epsilon to avoid log(0) issues\n",
    "        p = p + epsilon\n",
    "        q = q + epsilon\n",
    "        # Renormalise\n",
    "        p = p / p.sum(-1, keepdim=True)\n",
    "        q = q / q.sum(-1, keepdim=True)\n",
    "        \n",
    "        m = 0.5 * (p + q)\n",
    "\n",
    "        def kl(a, b):\n",
    "            return (a * (torch.log(a) - torch.log(b))).sum(-1)\n",
    "\n",
    "        return 0.5 * kl(p, m) + 0.5 * kl(q, m)\n",
    "\n",
    "    def set_teacher_blend(self, pert_logits, base_logits):\n",
    "        # Debug: check if logits are identical\n",
    "        are_identical = torch.allclose(pert_logits, base_logits, atol=1e-6)\n",
    "        if are_identical:\n",
    "            print(\"WARNING: Perturbed and base logits are identical!\")\n",
    "\n",
    "        # Distributions\n",
    "        p_base = torch.softmax(base_logits, dim=-1)\n",
    "        p_pert = torch.softmax(pert_logits, dim=-1)\n",
    "        \n",
    "        # JS divergence (batch-mean scalar)\n",
    "        divergence = self.js_divergence(p_pert, p_base).mean().item()\n",
    "        if np.isnan(divergence):\n",
    "            print(f'nan divergence: {divergence}')\n",
    "            divergence = 0.0\n",
    "            \n",
    "        # EMA\n",
    "        # Gain (high drift → backwards step, low drift → forwards step)\n",
    "        divergence = divergence * 100\n",
    "        js_tolerance = self.js_tolerance * 100\n",
    "\n",
    "        teacher_blend_percentage = ((js_tolerance - divergence) / js_tolerance)\n",
    "        teacher_blend_percentage = min(max(0.0, teacher_blend_percentage), 1.0)\n",
    "        self.teacher_blend = self.teacher_blend_max - (self.teacher_blend_max - self.teacher_blend_min) * teacher_blend_percentage\n",
    "\n",
    "        if self.debug:\n",
    "            print(f'Divergence: {divergence}, Teacher blend percentage: {teacher_blend_percentage}')\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_with_leash(self,\n",
    "                            prompt: str,\n",
    "                            max_new_tokens: int = 128,\n",
    "                            temperature: float = 0.7,\n",
    "                            top_p: float = 0.95,\n",
    "                            repetition_penalty: float = 1.15,\n",
    "                            only_new_tokens: bool = False):\n",
    "\n",
    "        input_ids = self.tokenizer(prompt, return_tensors='pt').to(self.device)['input_ids']\n",
    "        generated = input_ids.clone()\n",
    "        \n",
    "        base_past_key_values = None\n",
    "        perturbed_past_key_values = None\n",
    "\n",
    "        self.running_divergence = 0.0\n",
    "\n",
    "        # We run two forwards per step: base (no perturb), then perturbed (with current state)\n",
    "        for step in range(max_new_tokens):\n",
    "            # Determine input_ids for this step (only new tokens if using cache)\n",
    "            if base_past_key_values is None:\n",
    "                # First step: use full prompt\n",
    "                current_input_ids = input_ids\n",
    "            else:\n",
    "                # Subsequent steps: only use the last generated token\n",
    "                current_input_ids = generated[:, -1:]\n",
    "\n",
    "            # ---- BASE pass (perturbation off)\n",
    "            self.reset_to_base_model()\n",
    "            base_output = self.model(\n",
    "                input_ids=current_input_ids, \n",
    "                use_cache=True, \n",
    "                past_key_values=base_past_key_values, \n",
    "                output_hidden_states=True)\n",
    "            \n",
    "            base_logits = base_output.logits[:, -1, :]\n",
    "\n",
    "            # ---- PERTURBED pass (with perturbations)\n",
    "            self.apply_perturbation()\n",
    "            perturbed_output = self.model(\n",
    "                input_ids=current_input_ids, \n",
    "                use_cache=True, \n",
    "                past_key_values=perturbed_past_key_values,\n",
    "                output_hidden_states=True)\n",
    "\n",
    "            perturbed_logits = perturbed_output.logits[:, -1, :]\n",
    "\n",
    "            self.set_teacher_blend(perturbed_logits, base_logits)\n",
    "\n",
    "            base_past_key_values = base_output.past_key_values\n",
    "            perturbed_past_key_values = perturbed_output.past_key_values\n",
    "\n",
    "            # ---- Final logits for sampling: teacher blend\n",
    "            if self.debug:\n",
    "                print(f'teacher blend: {self.teacher_blend}')\n",
    "            final_logits = (1 - self.teacher_blend) * perturbed_logits + self.teacher_blend * base_logits\n",
    "\n",
    "            # ---- Sample\n",
    "            final_logits = final_logits / temperature\n",
    "\n",
    "            if repetition_penalty > 1.0:\n",
    "                unique_recent_tokens = set(generated[0, :].tolist())# A set of unique recent tokens for efficient lookup\n",
    "                \n",
    "                for token_id in unique_recent_tokens:\n",
    "                    #if token_id not in self.stop_words:\n",
    "                    # Apply the repetition penalty to only the tokens that have appeared recently\n",
    "                    if final_logits[0, token_id] < 0:\n",
    "                        final_logits[0, token_id] = final_logits[0, token_id] * repetition_penalty\n",
    "                    else:\n",
    "                        final_logits[0, token_id] = final_logits[0, token_id] / repetition_penalty\n",
    "\n",
    "\n",
    "            probs = torch.softmax(final_logits, dim=-1)\n",
    "            if top_p < 1.0:\n",
    "                sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "                cum = torch.cumsum(sorted_probs, dim=-1)\n",
    "                mask = cum > top_p\n",
    "                mask[..., 1:] = mask[..., :-1].clone()\n",
    "                mask[..., 0] = False\n",
    "                sorted_probs[mask] = 0\n",
    "                sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True).clamp_min(1e-9)\n",
    "                next_idx = torch.multinomial(sorted_probs, num_samples=1)\n",
    "                next_token = torch.gather(sorted_idx, -1, next_idx)\n",
    "            else:\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            if self.debug:\n",
    "                print(self.tokenizer.decode(next_token[0]))\n",
    "\n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "            if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        if only_new_tokens:\n",
    "            new_tokens = generated[:, input_ids.shape[-1]:]\n",
    "            llm_response = self.tokenizer.decode(new_tokens[0], skip_special_tokens=True)\n",
    "        else:\n",
    "            llm_response = self.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "        return llm_response\n",
    "\n",
    "    #dtype = torch.bfloat16\n",
    "    #force_eager_attention: bool = True force either way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c7da4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: As a friend describe what you see looking out at the sea\n",
      "Response: As a friend describe what you see looking out at the sea, I would say it's like someone has taken over an old photograph and put all of his artistry into this vast landscape.\n",
      "\n",
      "You can hear in your mind as he speaks with that hushed tone; for many years ago when we were young boys here on our island home underwater caverns where legend says the earth is filled up from below – but no one knows how they came to be or why there are soaring cliffs scattered across these picturesque vistas above ground level along its shores before going back down through rocky mountain ranges stretching off towards great distances away during recent times past centuries gone by\n"
     ]
    }
   ],
   "source": [
    "#|test\n",
    "model = LSDPerturbedLLM(\n",
    "    layer_start=5,                              # Start layer to perturb\n",
    "    layer_end=25,#30                               # Final layer to perturb  \n",
    "    attention_scaling_factor=1.7,\n",
    "    attention_noise=0.3,\n",
    "    attention_diagonal_penalty=-0.9,\n",
    "    attention_probability_smoothing_factor=0.5,\n",
    "    js_tolerance=0.3,                          # Tolerated JS divergence\n",
    "    teacher_blend_min=0.1,                     # Min teacher blend weight\n",
    "    teacher_blend_max=0.3,                     # Max teacher blend weight\n",
    ")\n",
    "\n",
    "# Generate text with the leash mechanism\n",
    "#prompt = 'You feel a slipping feeling pulling you in'#As a friend describe what you see looking out at the sea\n",
    "prompt = 'As a friend describe what you see looking out at the sea'\n",
    "#prompt = 'Why did we choose to go to the Moon? '\n",
    "#prompt = 'What do you see standing on the surface of the moon?'\n",
    "#prompt = 'What is the capital of France?'\n",
    "#prompt = 'What bear is best?'\n",
    "\n",
    "#prompt = 'You feel a slipping feeling pulling you in'\n",
    "#prompt = 'What is love?'\n",
    "#prompt = 'Why do all cultures share a fear of the dark?'\n",
    "\n",
    "print(f'Prompt: {prompt}')\n",
    "response = model.generate_with_leash(\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.9,#100\n",
    "    only_new_tokens=False\n",
    ")\n",
    "\n",
    "print(f'Response: {response}')\n",
    "\n",
    "\n",
    "\n",
    "#I am a person who is standing on an beach and I can describe in detail about how it feels to be able of being described as if Describing\n",
    "# 1. A flag, a manned by what or who is it that stands in front-what does one can I find out there and where are You may not be able to determine any information about something else than just an object; this question has no clear answer as its meaningless\n",
    "#The view of an endless expanse Of The ocean's vastness. From this vantage point 10 miles away At nighttime/night time-lapse (or) It feels like being in another worldly place(?)\n",
    "\n",
    "#So many parts of their brain firing all at once with no clear point, just all their synapses firing with no clear point\n",
    "#Overwhelmed by the concept\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5017e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "collected = gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_modification_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
