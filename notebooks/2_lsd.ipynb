{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp lsd\n",
    "#|export\n",
    "\n",
    "from perturbative_llm_cognition import core\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import types\n",
    "\n",
    "from torch import nn\n",
    "from transformers.models.mistral.modeling_mistral import apply_rotary_pos_emb, repeat_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37797a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class LSDPerturbedLLM:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        layer_start: int = 21,                                  # Start layer to perturb    \n",
    "        layer_end: int = 27,                                    # End layer to perturb\n",
    "        attention_scaling_factor: float = 1.4,                  # Scaling factor for the attention scores\n",
    "        attention_noise: float = 0.3,                           # Noise applied to the attention scores\n",
    "        attention_diagonal_penalty:float = 0.2,                 # Penalty for the diagonal attention scores\n",
    "        attention_probability_smoothing_factor: float = 0.5,    # Smoothing factor for the attention probs\n",
    "        js_tolerance: float = 0.2,                              # Tolerated JS divergence before leashing is maximised\n",
    "        perturbation_blend_min: float = 0.10,                   # Min perturbation blend \n",
    "        perturbation_blend_max: float = 0.50,                   # Max perturbation blend \n",
    "        debug: bool = False                                     # debug flag, when true debug information is printed\n",
    "        ):\n",
    "\n",
    "        # Initialises and validates target layers\n",
    "        self.layer_start = max(layer_start, 0)\n",
    "        self.layer_end = max(layer_end, 31)\n",
    "\n",
    "        # Initialises and validates leash parameters\n",
    "        self.js_tolerance = max(0.0, js_tolerance)\n",
    "        self.perturbation_blend_min = max(0.0, perturbation_blend_min)\n",
    "        self.perturbation_blend_max = min(1.0, perturbation_blend_max)\n",
    "\n",
    "        # Checks if the leash parameters are valid\n",
    "        if self.perturbation_blend_min > self.perturbation_blend_max:\n",
    "            raise ValueError('Invalid leash parameters')\n",
    "\n",
    "        # Initialises the attention parameters\n",
    "        self.attention_scaling_factor = attention_scaling_factor\n",
    "        self.attention_noise = attention_noise\n",
    "        self.attention_diagonal_penalty = attention_diagonal_penalty\n",
    "        self.attention_probability_smoothing_factor = attention_probability_smoothing_factor\n",
    "\n",
    "        # Initialises the model\n",
    "        self.tokenizer, self.model = core.load_tokenizer_and_model()\n",
    "        self.model.config.attn_implementation = 'eager'\n",
    "        self.model.config._attn_implementation = 'eager'  # Also try this\n",
    "        self.device = self.model.device\n",
    "\n",
    "        #Stores original attention functions\n",
    "        self._store_original_attention_functions()\n",
    "\n",
    "        # Debug flag\n",
    "        self.debug = debug\n",
    "\n",
    "    def _store_original_attention_functions(self):\n",
    "        \"\"\"Stores original attention forward functions\"\"\"\n",
    "        self.original_attention_forwards = {}\n",
    "        for i, block in enumerate(self.model.model.layers):\n",
    "            if self.is_target_layer(i):\n",
    "                self.original_attention_forwards[i] = block.self_attn.forward\n",
    "\n",
    "    def reset_to_base_model(self):\n",
    "        \"\"\"Reapplies the stored original functions for the unperturbed execution of the model as part of the leash mechanism\"\"\"\n",
    "        for i, block in enumerate(self.model.model.layers):\n",
    "            if self.is_target_layer(i) and i in self.original_attention_forwards:\n",
    "                block.self_attn.forward = self.original_attention_forwards[i]\n",
    "\n",
    "    def is_target_layer(self, index: int) -> bool:\n",
    "        \"\"\"Returns True if the layer is a target layer\"\"\"\n",
    "        return self.layer_start <= index < self.layer_end\n",
    "\n",
    "    def apply_perturbation(self):\n",
    "        \"\"\"Applies perturbations by replacing the attention forward function\"\"\"\n",
    "        for i, block in enumerate(self.model.model.layers):\n",
    "            if self.is_target_layer(i):\n",
    "                layer = block.self_attn\n",
    "                self.create_perturbed_forward(layer, i)\n",
    "\n",
    "    def create_perturbed_forward(self, layer, layer_idx):\n",
    "        \"\"\"Returns pertured attention forward function\"\"\"\n",
    "\n",
    "        # Retrieves the perturbation parameters\n",
    "        attention_scaling_factor = self.attention_scaling_factor\n",
    "        attention_noise = self.attention_noise\n",
    "        diag_penalty = self.attention_diagonal_penalty\n",
    "        attention_probability_smoothing_factor = self.attention_probability_smoothing_factor\n",
    "        debug = self.debug\n",
    "\n",
    "        # Creates the perturbed forward function\n",
    "        def forward(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            attention_mask=None,\n",
    "            position_ids=None,\n",
    "            past_key_value=None,\n",
    "            output_attentions=False,\n",
    "            use_cache=False,\n",
    "            cache_position=None,\n",
    "        ):\n",
    "            b, q_len, _ = hidden_states.size()\n",
    "            \n",
    "            # Query, Key, Value projections\n",
    "            q = self.q_proj(hidden_states).view(b, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            k = self.k_proj(hidden_states).view(b, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "            v = self.v_proj(hidden_states).view(b, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "            \n",
    "            # Applies Rotary Positional Embeddings (RoPE) to the query and key\n",
    "            seq_len = v.shape[-2]\n",
    "            cos, sin = self.rotary_emb(\n",
    "                v,\n",
    "                position_ids if position_ids is not None else torch.arange(seq_len, device=v.device).unsqueeze(0))\n",
    "            q, k = apply_rotary_pos_emb(q, k, cos, sin, position_ids=None)\n",
    "            \n",
    "            # Cache update\n",
    "            if past_key_value is not None:\n",
    "                k, v = past_key_value.update(k, v, layer_idx, {'sin': sin, 'cos': cos, 'cache_position': cache_position})\n",
    "            \n",
    "            # Expand KV\n",
    "            k = repeat_kv(k, self.num_key_value_groups)\n",
    "            v = repeat_kv(v, self.num_key_value_groups)\n",
    "            \n",
    "            # Attention logits\n",
    "            attention_scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "            if attention_mask is not None:\n",
    "                attention_scores = attention_scores + attention_mask[:, :, :, : k.size(-2)]\n",
    "            \n",
    "            # Perturbation modifications \n",
    "\n",
    "            # Scales the attention scores (modification)\n",
    "            attention_scores = attention_scores / attention_scaling_factor\n",
    "\n",
    "            # Adds Gaussian noise\n",
    "            noise = torch.randn_like(attention_scores) * attention_noise\n",
    "            attention_scores = attention_scores + noise\n",
    "\n",
    "            # Applies diagonal penalty (modification)\n",
    "            if attention_scores.shape[-2] == 1:\n",
    "                attention_scores[..., -1] = attention_scores[..., -1] - diag_penalty\n",
    "\n",
    "            # Calculates the attention probs\n",
    "            attention_probs = nn.functional.softmax(attention_scores, dim=-1).to(q.dtype)\n",
    "\n",
    "            # Flatten distributions (modification)\n",
    "            attention_probs = attention_probs ** attention_probability_smoothing_factor\n",
    "            attention_probs = attention_probs / attention_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            attention_probs = nn.functional.dropout(attention_probs, p=self.attention_dropout, training=self.training)#Disabled\n",
    "            \n",
    "            attention_output = torch.matmul(attention_probs, v).transpose(1, 2).contiguous().view(b, q_len, -1)\n",
    "            attention_output = self.o_proj(attention_output)\n",
    "            \n",
    "            if debug:\n",
    "                print(f'[L{layer_idx}] attention_scaling_factor={attention_scaling_factor} attention_noise={attention_noise} diag={diag_penalty}')\n",
    "            \n",
    "            return (\n",
    "                attention_output,\n",
    "                (attention_probs if output_attentions else None),\n",
    "                (past_key_value if use_cache else None),\n",
    "            )\n",
    "        \n",
    "        # Binds the perturbed forward function to the layer\n",
    "        layer.forward = types.MethodType(forward, layer)\n",
    "        return layer\n",
    "\n",
    "    @staticmethod\n",
    "    def js_divergence(p, q, epsilon=1e-6, threshold=1e-6):\n",
    "        mask = (p > threshold) | (q > threshold)\n",
    "    \n",
    "        if mask.sum() == 0:\n",
    "            raise ValueError('No valid probs found in input distributions')\n",
    "        \n",
    "        p = p[mask]\n",
    "        q = q[mask]\n",
    "\n",
    "        # Adds small epsilon to avoid log(0) issues\n",
    "        p = p + epsilon\n",
    "        q = q + epsilon\n",
    "        # Renormalises values\n",
    "        p = p / p.sum(-1, keepdim=True)\n",
    "        q = q / q.sum(-1, keepdim=True)\n",
    "        \n",
    "        m = 0.5 * (p + q)\n",
    "\n",
    "        def kl_divergence(a, b):\n",
    "            return (a * (torch.log(a) - torch.log(b))).sum(-1)\n",
    "\n",
    "        return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "\n",
    "    def set_perturbation_blend(self, pert_logits, base_logits):\n",
    "        # Raises an error if the perturbed and base logits are identical\n",
    "        are_identical = torch.allclose(pert_logits, base_logits, atol=1e-6)\n",
    "        if are_identical:\n",
    "            raise ValueError(\"WARNING: Perturbed and base logits are identical!\")\n",
    "\n",
    "        # Probability Distributions\n",
    "        p_base = torch.softmax(base_logits, dim=-1)\n",
    "        p_pert = torch.softmax(pert_logits, dim=-1)\n",
    "        \n",
    "        # JS divergence \n",
    "        divergence = self.js_divergence(p_pert, p_base).mean().item()\n",
    "        if np.isnan(divergence):\n",
    "            print(f'nan divergence: {divergence}')\n",
    "            divergence = 0.0\n",
    "            \n",
    "        # Blend ratio based on the JS divergence\n",
    "        divergence = divergence\n",
    "        js_tolerance = self.js_tolerance\n",
    "\n",
    "        perturbation_blend_percentage = ((js_tolerance - divergence) / js_tolerance)\n",
    "        perturbation_blend_percentage = min(max(0.0, perturbation_blend_percentage), 1.0)\n",
    "        self.perturbation_blend = self.perturbation_blend_max - (self.perturbation_blend_max - self.perturbation_blend_min) * perturbation_blend_percentage\n",
    "\n",
    "        if self.debug:\n",
    "            print(f'Divergence: {divergence}, Perturbation blend percentage: {perturbation_blend_percentage}')\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_with_leash(self,\n",
    "                            prompt: str,\n",
    "                            max_new_tokens: int = 128,\n",
    "                            temperature: float = 0.7,\n",
    "                            top_p: float = 0.95,\n",
    "                            repetition_penalty: float = 1.15,\n",
    "                            only_new_tokens: bool = False):\n",
    "\n",
    "        input_ids = self.tokenizer(prompt, return_tensors='pt').to(self.device)['input_ids']\n",
    "        generated = input_ids.clone()\n",
    "        \n",
    "        base_past_key_values = None\n",
    "        perturbed_past_key_values = None\n",
    "\n",
    "        self.running_divergence = 0.0\n",
    "\n",
    "        # Runs the model for the maximum number of new tokens\n",
    "        for step in range(max_new_tokens):\n",
    "            # Determines input_ids for this step\n",
    "            if base_past_key_values is None:\n",
    "                current_input_ids = input_ids# Full prompt is used for the first step\n",
    "            else:\n",
    "                current_input_ids = generated[:, -1:]# Only the last generated token is used for subsequent steps\n",
    "\n",
    "            # The model is executed twice as part of the leash mechanism\n",
    "            # Unperturbed execution\n",
    "            self.reset_to_base_model()\n",
    "            base_output = self.model(\n",
    "                input_ids=current_input_ids, \n",
    "                use_cache=True, \n",
    "                past_key_values=base_past_key_values, \n",
    "                output_hidden_states=True)\n",
    "            \n",
    "            base_logits = base_output.logits[:, -1, :]\n",
    "\n",
    "            # Perturbed execution\n",
    "            self.apply_perturbation()\n",
    "            perturbed_output = self.model(\n",
    "                input_ids=current_input_ids, \n",
    "                use_cache=True, \n",
    "                past_key_values=perturbed_past_key_values,\n",
    "                output_hidden_states=True)\n",
    "\n",
    "            perturbed_logits = perturbed_output.logits[:, -1, :]\n",
    "\n",
    "            # Updates the past key values\n",
    "            base_past_key_values = base_output.past_key_values\n",
    "            perturbed_past_key_values = perturbed_output.past_key_values            \n",
    "            \n",
    "            # Perturbation blend calculated and applied to create final logits\n",
    "            self.set_perturbation_blend(perturbed_logits, base_logits)\n",
    "            final_logits = (1 - self.perturbation_blend) * perturbed_logits + self.perturbation_blend * base_logits\n",
    "\n",
    "            if self.debug:\n",
    "                print(f'Perturbation blend: {self.perturbation_blend}')\n",
    "\n",
    "            # Applies temperature to the final logits\n",
    "            final_logits = final_logits / temperature\n",
    "\n",
    "            # Applies repetition penalty to the final logits\n",
    "            if repetition_penalty > 1.0:\n",
    "                unique_recent_tokens = set(generated[0, :].tolist())# A set of unique recent tokens for efficient lookup\n",
    "                \n",
    "                for token_id in unique_recent_tokens:\n",
    "                    if final_logits[0, token_id] < 0:\n",
    "                        final_logits[0, token_id] = final_logits[0, token_id] * repetition_penalty\n",
    "                    else:\n",
    "                        final_logits[0, token_id] = final_logits[0, token_id] / repetition_penalty\n",
    "\n",
    "\n",
    "            probs = torch.softmax(final_logits, dim=-1)\n",
    "\n",
    "            # Applies top-p sampling to the final logits\n",
    "            if top_p < 1.0:\n",
    "                sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "                cum = torch.cumsum(sorted_probs, dim=-1)\n",
    "                mask = cum > top_p\n",
    "                mask[..., 1:] = mask[..., :-1].clone()\n",
    "                mask[..., 0] = False\n",
    "                sorted_probs[mask] = 0\n",
    "                sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True).clamp_min(1e-9)\n",
    "                next_idx = torch.multinomial(sorted_probs, num_samples=1)\n",
    "                next_token = torch.gather(sorted_idx, -1, next_idx)\n",
    "            else:\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            if self.debug:\n",
    "                print(f'Next token: {self.tokenizer.decode(next_token[0])}')\n",
    "\n",
    "            generated = torch.cat([generated, next_token], dim=-1)#Generated tokens are updated\n",
    "\n",
    "            # Breaks if the end of the sequence token is generated\n",
    "            if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        # Decodes the generated tokens\n",
    "        # Only returns the new tokens if the flag is set\n",
    "        if only_new_tokens:\n",
    "            new_tokens = generated[:, input_ids.shape[-1]:]\n",
    "            llm_response = self.tokenizer.decode(new_tokens[0], skip_special_tokens=True)\n",
    "        else:\n",
    "            llm_response = self.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "        return llm_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7da4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|test\n",
    "model = LSDPerturbedLLM(\n",
    "    layer_start=5,\n",
    "    layer_end=25,\n",
    "    attention_scaling_factor=1.7,\n",
    "    attention_noise=0.3,\n",
    "    attention_diagonal_penalty=-0.9,\n",
    "    attention_probability_smoothing_factor=0.5,\n",
    "    js_tolerance=0.3,\n",
    "    perturbation_blend_min=0.1,\n",
    "    perturbation_blend_max=0.3,\n",
    ")\n",
    "\n",
    "prompt = 'As a friend describe what you see looking out at the sea'\n",
    "\n",
    "print(f'Prompt: {prompt}')\n",
    "response = model.generate_with_leash(\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.9,#100\n",
    "    only_new_tokens=True\n",
    ")\n",
    "\n",
    "print(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5017e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "collected = gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_modification_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
