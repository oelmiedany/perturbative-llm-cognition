{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp lsd\n",
    "#|export\n",
    "\n",
    "from perturbative_llm_cognition import core\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37797a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class LSDPerturbedLLM:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        layer_start: int = 21,                              # Start layer to perturb    \n",
    "        layer_end: int = 30,                                # Final layer to perturb\n",
    "        attention_temperature_target: float = 1.20,         # Temperature > 1 flattens attention (scores /= temperature)\n",
    "        attention_diagonal_penalty_target: float = 0.40,    # Subtract at most-recent key at decode (q_len==1)\n",
    "        swiglu_skew_target: float = 0.15,                   # Magnitude skew exponent\n",
    "        swiglu_noise_target: float = 0.12,                  # Structured noise scale (zero-mean) <- better name required\n",
    "        js_tolerance: float = 0.12,                         # Tolerated JS divergence before clamping gets strong\n",
    "        js_softness: float = 0.02,                          # Sigmoid softness around the tolerance <- better name required\n",
    "        divergence_smoothing: float = 0.8,                              # EMA smoothing for JS\n",
    "        teacher_blend_min: float = 0.05,                    # Min teacher blend weight\n",
    "        teacher_blend_max: float = 0.60,                    # Max teacher blend weight when drift is high\n",
    "        ):\n",
    "\n",
    "        self.layer_start = max(layer_start, 0)\n",
    "        self.layer_end = max(layer_end, 31)\n",
    "        self.attention_temperature_target = max(1.0, attention_temperature_target)\n",
    "        self.attention_diagonal_penalty_target = max(0.0, attention_diagonal_penalty_target)\n",
    "        self.swiglu_skew_target = max(0.0, swiglu_skew_target)\n",
    "        self.swiglu_noise_target = max(0.0, swiglu_noise_target)\n",
    "        self.js_tolerance = max(0.0, js_tolerance)\n",
    "        self.js_softness = max(0.0, js_softness)\n",
    "        self.divergence_smoothing = max(0.0, divergence_smoothing)\n",
    "        self.teacher_blend_min = max(0.0, teacher_blend_min)\n",
    "        self.teacher_blend_max = min(1.0, teacher_blend_max)\n",
    "\n",
    "        if self.teacher_blend_min > self.teacher_blend_max:\n",
    "            raise ValueError('Invalid teacher parameters')\n",
    "\n",
    "        self.set_perturbation_parameters(\n",
    "            attention_temperature=attention_temperature_target,\n",
    "            attention_diagonal_penalty=attention_diagonal_penalty_target,\n",
    "            teacher_blend=teacher_blend_min,\n",
    "            #swiglu_skew_target=swiglu_skew_target,\n",
    "            #swiglu_noise_target=swiglu_noise_target,\n",
    "        )\n",
    "\n",
    "        self.tokenizer, self.model = core.load_tokenizer_and_model()\n",
    "        self.model.config.attn_implementation = 'eager'\n",
    "        self.model.config._attn_implementation = 'eager'  # Also try this\n",
    "        self.device = self.model.device\n",
    "\n",
    "        self._store_original_attention_functions()\n",
    "\n",
    "    def set_perturbation_parameters(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Clamps all perturbation parameters to valid ranges.\n",
    "        \"\"\"\n",
    "\n",
    "        for parameter, value in kwargs.items():\n",
    "            if parameter in ['attention_temperature']:\n",
    "                setattr(self, parameter, max(1.0, value))\n",
    "            elif parameter in ['teacher_blend']:\n",
    "                setattr(self, parameter, max(0.0, min(1.0, value)))\n",
    "            else:\n",
    "                setattr(self, parameter, max(0.0, value))\n",
    "\n",
    "    def _store_original_attention_functions(self):\n",
    "        \"\"\"Store original attention forward functions for reset capability\"\"\"\n",
    "        self.original_attention_forwards = {}\n",
    "        for i, block in enumerate(self.model.model.layers):\n",
    "            if self.is_target_layer(i):\n",
    "                self.original_attention_forwards[i] = block.self_attn.forward\n",
    "\n",
    "    def reset_to_base_model(self):\n",
    "        \"\"\"Completely reset model to base state by restoring original functions\"\"\"\n",
    "        for i, block in enumerate(self.model.model.layers):\n",
    "            if self.is_target_layer(i) and i in self.original_attention_forwards:\n",
    "                block.self_attn.forward = self.original_attention_forwards[i]\n",
    "\n",
    "    def is_target_layer(self, index: int) -> bool:\n",
    "        return self.layer_start <= index < self.layer_end\n",
    "\n",
    "    def apply_perturbation(self):\n",
    "        \"\"\"Apply perturbations while preserving original functions\"\"\"\n",
    "        # Always recreate the perturbation functions to get updated parameters\n",
    "        self.apply_attention_perturbation()\n",
    "\n",
    "\n",
    "    def apply_attention_perturbation(self):\n",
    "        \"\"\"Apply perturbations by directly modifying attention weights\"\"\"\n",
    "        \n",
    "        for i, block in enumerate(self.model.model.layers):\n",
    "            if not self.is_target_layer(i):\n",
    "                continue\n",
    "                \n",
    "            print(f\"Applying perturbation to layer {i}\")\n",
    "            \n",
    "            # Store original forward\n",
    "            original_forward = block.self_attn.forward\n",
    "            \n",
    "            def create_perturbed_forward(original_forward, layer_idx, self_ref):\n",
    "                def perturbed_forward(*args, **kwargs):\n",
    "                    print(f\"DEBUG: Perturbed attention called for layer {layer_idx}\")\n",
    "                    \n",
    "                    # Call the original forward to get the result\n",
    "                    result = original_forward(*args, **kwargs)\n",
    "                    \n",
    "                    # If we got attention weights, modify them\n",
    "                    if isinstance(result, tuple) and len(result) >= 2:\n",
    "                        \n",
    "                        if len(result) == 2:\n",
    "                            attn_output, attn_weights = result\n",
    "                            past_key_value = None\n",
    "                        else:\n",
    "                            attn_output, attn_weights, past_key_value = result\n",
    "\n",
    "                        if attn_weights is not None:\n",
    "                            print(f\"Found attention weights with shape: {attn_weights.shape}\")\n",
    "                            \n",
    "                            # Apply your modifications to the attention weights\n",
    "                            modified_weights = attn_weights.clone()\n",
    "                            \n",
    "                            # Temperature scaling\n",
    "                            modified_weights = modified_weights / self.attention_temperature\n",
    "                            \n",
    "                            # Diagonal penalty for decode step\n",
    "                            if modified_weights.size(-2) == 1:\n",
    "                                modified_weights[..., -1] = modified_weights[..., -1] - self.attention_diagonal_penalty\n",
    "\n",
    "                            # Apply softmax\n",
    "                            attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "                            \n",
    "                            # Apply attention to values\n",
    "                            attn_output = torch.matmul(attn_weights, value_states)\n",
    "                            \n",
    "                            # Reshape back\n",
    "                            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, num_heads * head_dim)\n",
    "                            \n",
    "                            # Apply output projection\n",
    "                            attn_output = attn_layer.o_proj(attn_output)\n",
    "                            \n",
    "                            \n",
    "                            print(f\"Applied modifications: tau={self.attention_temperature}, diag={self_ref.attention_diagonal_penalty}\")\n",
    "                            \n",
    "                            # Return modified result\n",
    "                            if past_key_value is not None:\n",
    "                                return attn_output, modified_weights, past_key_value\n",
    "                            else:\n",
    "                                return attn_output, modified_weights\n",
    "                    \n",
    "                    return result\n",
    "                        \n",
    "                return perturbed_forward\n",
    "            \n",
    "            block.self_attn.forward = create_perturbed_forward(original_forward, i, self)\n",
    "\n",
    "    @staticmethod\n",
    "    def js_divergence(p, q, eps=1e-8):\n",
    "        # Add small epsilon to avoid log(0) issues\n",
    "        p = p + eps\n",
    "        q = q + eps\n",
    "        \n",
    "        # Renormalise\n",
    "        p = p / p.sum(-1, keepdim=True)\n",
    "        q = q / q.sum(-1, keepdim=True)\n",
    "        \n",
    "        m = 0.5 * (p + q)\n",
    "        def kl(a, b):\n",
    "            return (a * (torch.log(a) - torch.log(b))).sum(-1)\n",
    "        return 0.5 * kl(p, m) + 0.5 * kl(q, m)\n",
    "\n",
    "    def step(self, pert_logits, base_logits):\n",
    "        # Debug: check if logits are identical\n",
    "        are_identical = torch.allclose(pert_logits, base_logits, atol=1e-6)\n",
    "        if are_identical:\n",
    "            print(\"WARNING: Perturbed and base logits are identical!\")\n",
    "        \n",
    "        # Distributions\n",
    "        p_base = torch.softmax(base_logits, dim=-1)\n",
    "        p_pert = torch.softmax(pert_logits, dim=-1)\n",
    "        # JS divergence (batch-mean scalar)\n",
    "        d = self.js_divergence(p_pert, p_base).mean().item()\n",
    "        print(f\"JS divergence: {d}\")\n",
    "        # EMA\n",
    "        beta = self.divergence_smoothing\n",
    "        self.running_divergence = beta * self.running_divergence + (1 - beta) * d\n",
    "        # Gain (higher drift â†’ smaller g)\n",
    "        gain = torch.sigmoid(torch.tensor((self.js_tolerance - self.running_divergence) / self.js_softness)).item()\n",
    "\n",
    "        # Effective strengths\n",
    "        attention_temperature  = 1.0 + (self.attention_temperature_target - 1.0) * gain\n",
    "        attention_diagonal_penalty = self.attention_diagonal_penalty_target * gain\n",
    "        #skew_eff = self.cfg.swiglu_skew_target * g\n",
    "        #noise_eff= self.cfg.swiglu_noise_target * g\n",
    "\n",
    "        # Teacher blend increases as drift grows (g small)\n",
    "        teacher_blend = self.teacher_blend_min + (1 - gain) * (self.teacher_blend_max - self.teacher_blend_min)\n",
    "        return {\n",
    "            \"attention_temperature\": float(attention_temperature),\n",
    "            \"attention_diagonal_penalty\": float(attention_diagonal_penalty),\n",
    "            #\"skew_eff\": float(skew_eff),\n",
    "            #\"noise_eff\": float(noise_eff),\n",
    "            \"teacher_blend\": float(teacher_blend),  # Fixed parameter name\n",
    "            #\"ema_js\": float(self.running_divergence),\n",
    "            #\"gain\": float(gain),\n",
    "        }\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_with_leash(self,\n",
    "                            prompt: str,\n",
    "                            max_new_tokens: int = 128,\n",
    "                            temperature: float = 0.7,\n",
    "                            top_p: float = 0.95,\n",
    "                            repetition_penalty: float = 1.15):\n",
    "\n",
    "\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)['input_ids']\n",
    "        generated = input_ids\n",
    "        past_key_values = None\n",
    "\n",
    "        self.running_divergence = 0.0\n",
    "\n",
    "        # We run two forwards per step: base (no perturb), then perturbed (with current state)\n",
    "        for step in range(max_new_tokens):\n",
    "            # ---- BASE pass (perturbation off)\n",
    "            print(f'base temperature: {self.attention_temperature}')\n",
    "            self.reset_to_base_model()\n",
    "            base_output = self.model(\n",
    "                input_ids=generated, \n",
    "                use_cache=True, \n",
    "                past_key_values=past_key_values, \n",
    "                output_hidden_states=True)\n",
    "            \n",
    "            base_logits = base_output.logits[:, -1, :]\n",
    "\n",
    "            self.apply_perturbation()\n",
    "            print(f'perturbed temperature: {self.attention_temperature}')\n",
    "            perturbed_output = self.model(\n",
    "                input_ids=generated, \n",
    "                use_cache=True, \n",
    "                past_key_values=past_key_values, \n",
    "                output_hidden_states=True)\n",
    "\n",
    "            perturbed_logits = perturbed_output.logits[:, -1, :]\n",
    "\n",
    "            next_step = self.step(perturbed_logits, base_logits)\n",
    "            self.set_perturbation_parameters(**next_step)\n",
    "            print(self.attention_temperature, self.attention_diagonal_penalty, self.teacher_blend)\n",
    "            past_key_values = base_output.past_key_values\n",
    "\n",
    "            # ---- Final logits for sampling: teacher blend\n",
    "            final_logits = (1 - self.teacher_blend) * perturbed_logits + self.teacher_blend * base_logits\n",
    "\n",
    "            # ---- Sample\n",
    "            final_logits = final_logits / max(1e-5, temperature)\n",
    "\n",
    "            if repetition_penalty != 1.0:\n",
    "                # Get recent tokens (last 50 tokens)\n",
    "                recent_tokens = generated[0, -50:] if generated.shape[1] > 50 else generated[0]\n",
    "                for token_id in recent_tokens:\n",
    "                    if final_logits[0, token_id] < 0:\n",
    "                        final_logits[0, token_id] *= repetition_penalty\n",
    "                    else:\n",
    "                        final_logits[0, token_id] /= repetition_penalty\n",
    "\n",
    "            \n",
    "            probs = torch.softmax(final_logits, dim=-1)\n",
    "            if top_p < 1.0:\n",
    "                sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "                cum = torch.cumsum(sorted_probs, dim=-1)\n",
    "                mask = cum > top_p\n",
    "                mask[..., 1:] = mask[..., :-1].clone()\n",
    "                mask[..., 0] = False\n",
    "                sorted_probs[mask] = 0\n",
    "                sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True).clamp_min(1e-9)\n",
    "                next_idx = torch.multinomial(sorted_probs, num_samples=1)\n",
    "                next_token = torch.gather(sorted_idx, -1, next_idx)\n",
    "            else:\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "        return self.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    #dtype = torch.bfloat16\n",
    "    #force_eager_attention: bool = True force either way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7da4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|test\n",
    "model = LSDPerturbedLLM(\n",
    "    layer_start=21,                              # Start layer to perturb\n",
    "    layer_end=30,                               # Final layer to perturb  \n",
    "    attention_temperature_target=1.40,          # Temperature > 1 flattens attention\n",
    "    attention_diagonal_penalty_target=0.40,     # Reduced penalty for most recent key\n",
    "    js_tolerance=0.25,                          # Tolerated JS divergence\n",
    "    teacher_blend_min=0.10,                     # Min teacher blend weight\n",
    "    teacher_blend_max=0.60,                     # Max teacher blend weight\n",
    ")\n",
    "\n",
    "# Generate text with the leash mechanism\n",
    "prompt = 'Describe what you see looking out at the sea using a 100 word sentence'\n",
    "print(f'Prompt: {prompt}')\n",
    "response = model.generate_with_leash(\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.5\n",
    ")\n",
    "\n",
    "print(f'Response: {response}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_modification_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
