# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/1_core.ipynb.

# %% auto 0
__all__ = ['MODEL_ID', 'load_tokenizer_and_model']

# %% ../notebooks/1_core.ipynb 0
import argparse, torch, random
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# %% ../notebooks/1_core.ipynb 1
MODEL_ID = 'mistralai/Mistral-7B-Instruct-v0.3'

# %% ../notebooks/1_core.ipynb 2
def load_tokenizer_and_model():

    bnb = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type='nf4',
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb,
        device_map="auto",
        trust_remote_code=False,
    )

    model.eval()
    model.config.use_cache = True

    return tokenizer, model

