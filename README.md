The link between large language models (LLMs) and the human brain has been reinforced in both technical and colloquial discourse. This association arises from several parallels: LLMs are built on neural networks, an abstraction of biological neurons; their outputs are estimations of human expression; and their very branding often draws on the language of “thought.” In this sense, LLMs have become symbolic of artificial thinking.

The metaphor runs deeper. Both LLMs and brains can be seen as black boxes whose inner workings are understood primarily through patterns of activity. For instance, some attention heads in LLMs are more active on specific topics, just as brain regions light up in response to certain concepts. Yet the analogy quickly frays as LLMs are engineered for a narrowly defined task, while human cognition is broader and more flexible.

This project aims to extend the metaphor, pushing it to its breaking point as a way of critiquing how we perceive LLMs. It explores the difficulty of reproducing the quirks and nonlinear creativity of biological minds, particularly in the context of psychedelics. Psychedelics alter brain activity in ways that inspire novel associations, concepts, and perspectives. By asking what changes would be required to make an LLM behave in a comparable fashion, we can illuminate both the limits of equating token prediction with “thought” and the creative potential of these models when reframed outside that paradigm.

The project is organised around specific drugs, examining how each alters the brain, how parallel modifications might be implemented in LLMs, and how the resulting outputs diverge from biological cognition, yet still generate surprising and sometimes useful insights.

The first case study is LSD. Lysergic acid diethylamide weakens the thalamic filter, increasing the sensory input reaching higher cortical regions. At the same time, activation of 5-HT2A receptors dampens the default mode network, producing a state where some areas of the brain are hyper-stimulated while their usual connections are suppressed. The result is often described as pushing the brain to the “edge of chaos”.

The experiment, then, is to push an LLM towards a similar edge, relaxing its conventional pathways, disrupting its default modes of association, and encouraging novel connections. In doing so, we create not an artificial psychedelic experience, but a provocative demonstration of how models and brains behave differently when coaxed into unfamiliar territory. While highlighting the fragility of the brain/LLM analogy it also serves to demonstrates the surprising originality hidden within these systems.
