# Perturbative LLM Cognition 

This repository accompanies an article that examines the often-implied relationship between large language models and human thought by deliberately pushing that assumption to failure. Inspired by how psychedelics alter cognitive processing in the brain, this project asks what kinds of internal modifications would be required to provoke comparable shifts in a transformer model’s behaviour, and what those shifts reveal about the nature of language generation when the model’s trained assumptions are suspended.


Read the full article [here](https://medium.com/@omarelmiedany/language-without-an-end-psychedelic-perturbations-in-large-language-models-b4d30380e69d)

## To try the modified model
1. Clone the Repository
2. Install dependencies
``` sh
$ pip install -e .
```
3.  CD to experiments and run [`perturbed_chat.py`](./experiments/perturbed_chat.py)